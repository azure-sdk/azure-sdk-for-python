# pylint: disable=too-many-lines,too-many-statements
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from io import IOBase
import json
import sys
from typing import Any, AsyncIterable, Callable, Dict, IO, List, Optional, TypeVar, Union, overload
import urllib.parse

from azure.core.async_paging import AsyncItemPaged, AsyncList
from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    map_error,
)
from azure.core.pipeline import PipelineResponse
from azure.core.rest import AsyncHttpResponse, HttpRequest
from azure.core.tracing.decorator import distributed_trace
from azure.core.tracing.decorator_async import distributed_trace_async
from azure.core.utils import case_insensitive_dict

from ... import models as _models
from ..._model_base import SdkJSONEncoder, _deserialize
from ...operations._operations import (
    build_registrations_create_or_replace_registration_request,
    build_registrations_delete_registration_request,
    build_registrations_get_registration_request,
    build_registrations_list_registrations_request,
)

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
JSON = MutableMapping[str, Any]  # pylint: disable=unsubscriptable-object
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]


class RegistrationsOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~speechingestion.aio.IngestionClient`'s
        :attr:`registrations` attribute.
    """

    def __init__(self, *args, **kwargs) -> None:
        input_args = list(args)
        self._client = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @distributed_trace_async
    async def get_registration(self, name: str, **kwargs: Any) -> _models.Registration:
        # pylint: disable=line-too-long
        """Fetch a Registration by name.

        :param name: The unique name of the registration resource. Required.
        :type name: str
        :return: Registration. The Registration is compatible with MutableMapping
        :rtype: ~speechingestion.models.Registration
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "batchTranscriptionModel": {
                        "customDomain": "str",  # Custom domain name for batch transcription.
                          Required.
                        "locale": "str",  # Optional. The expected locale of the audio data
                          to transcribe. For example, 'en-US', 'fr-FR', 'zh-CN', etc. For a full list
                          of supported locales, see `Supported languages and regions
                          <https://learn.microsoft.com/azure/ai-services/speech-service/language-support?tabs=stt>`_.
                        "modelX": {
                            "self": "str"  # self. Required.
                        },
                        "transcriptionProperties": {
                            "channels": [
                                0  # Optional. An array of channel numbers to
                                  process. Channels 0 and 1 are transcribed by default.
                            ],
                            "diarization": {
                                "speakers": {
                                    "maxCount": 0,  # Optional. Maximum count of
                                      speakers.
                                    "minCount": 0  # Optional. Minimum count of
                                      speakers.
                                }
                            },
                            "diarizationEnabled": bool,  # Optional. Specifies that the
                              Speech service should attempt diarization analysis on the input, which is
                              expected to be a mono channel that contains two voices. The default value
                              is false.
                            "displayFormWordLevelTimestampsEnabled": bool,  # Optional.
                              Specifies whether to include word-level timestamps on the display form of
                              the transcription results. The results are returned in the displayWords
                              property of the transcription file. The default value is false.
                            "languageIdentification": {
                                "candidateLocales": [
                                    "str"  # candidateLocales. Required.
                                ],
                                "mode": "str",  # mode. Required.
                                "speechModelMapping": {
                                    "str": {
                                        "self": "str"  # self. Required.
                                    }
                                }
                            },
                            "profanityFilterMode": "str",  # Optional. Specifies how to
                              handle profanity in recognition results. Accepted values are None to
                              disable profanity filtering, Masked to replace profanity with asterisks,
                              Removed to remove all profanity from the result, or Tags to add profanity
                              tags. The default value is Masked.
                            "punctuationMode": "str",  # Optional. Specifies how to
                              handle punctuation in recognition results. Accepted values are None to
                              disable punctuation, Dictated to imply explicit (spoken) punctuation,
                              Automatic to let the decoder deal with punctuation, or
                              DictatedAndAutomatic to use dictated and automatic punctuation. The
                              default value is DictatedAndAutomatic.
                            "wordLevelTimestampsEnabled": bool  # Optional. Specifies if
                              word level timestamps should be included in the output. The default value
                              is false.
                        }
                    },
                    "name": "str",  # The unique name of the registration resource. Required.
                    "processingModel": {
                        "deliveryMode": "str",  # deliveryMode. Required.
                        "subjectEndsWith": "str",  # Optional. Extensions of input files.
                        "systemTopicResourceId": "str"  # Optional. ResourceId for System
                          Topic.
                    },
                    "statusModel": {
                        "code": "str",  # Status code. Required.
                        "message": "str"  # Optional. Message.
                    },
                    "storageModel": {
                        "blobStorageEndpoint": "str",  # The Uri of the blob storage account
                          in the form of 'https://:code:`<accountname>`.blob.core.windows.net'.
                          Required.
                        "errorOutputContainerName": "str",  # The name of the container that
                          saves the error messages during processing. Required.
                        "inputContainerName": "str",  # The name of the container that holds
                          the audio files to be processed. Required.
                        "outputContainerName": "str",  # The name of the container that
                          stores the transcription and analytics results. Required.
                        "processedContainerName": "str"  # The name of the container that
                          archives the audio files that have been transcribed. Required.
                    },
                    "analyticsModel": {
                        "endpoint": "str"  # The analytics endpoint to call. Required.
                    }
                }
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.Registration] = kwargs.pop("cls", None)

        _request = build_registrations_get_registration_request(
            name=name,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                await response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.Registration, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def create_or_replace_registration(
        self, name: str, resource: _models.Registration, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Registration:
        # pylint: disable=line-too-long
        """Creates or replaces a Registration.

        :param name: The unique name of the registration resource. Required.
        :type name: str
        :param resource: The resource instance. Required.
        :type resource: ~speechingestion.models.Registration
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Registration. The Registration is compatible with MutableMapping
        :rtype: ~speechingestion.models.Registration
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # JSON input template you can fill out and use as your body input.
                resource = {
                    "batchTranscriptionModel": {
                        "customDomain": "str",  # Custom domain name for batch transcription.
                          Required.
                        "locale": "str",  # Optional. The expected locale of the audio data
                          to transcribe. For example, 'en-US', 'fr-FR', 'zh-CN', etc. For a full list
                          of supported locales, see `Supported languages and regions
                          <https://learn.microsoft.com/azure/ai-services/speech-service/language-support?tabs=stt>`_.
                        "modelX": {
                            "self": "str"  # self. Required.
                        },
                        "transcriptionProperties": {
                            "channels": [
                                0  # Optional. An array of channel numbers to
                                  process. Channels 0 and 1 are transcribed by default.
                            ],
                            "diarization": {
                                "speakers": {
                                    "maxCount": 0,  # Optional. Maximum count of
                                      speakers.
                                    "minCount": 0  # Optional. Minimum count of
                                      speakers.
                                }
                            },
                            "diarizationEnabled": bool,  # Optional. Specifies that the
                              Speech service should attempt diarization analysis on the input, which is
                              expected to be a mono channel that contains two voices. The default value
                              is false.
                            "displayFormWordLevelTimestampsEnabled": bool,  # Optional.
                              Specifies whether to include word-level timestamps on the display form of
                              the transcription results. The results are returned in the displayWords
                              property of the transcription file. The default value is false.
                            "languageIdentification": {
                                "candidateLocales": [
                                    "str"  # candidateLocales. Required.
                                ],
                                "mode": "str",  # mode. Required.
                                "speechModelMapping": {
                                    "str": {
                                        "self": "str"  # self. Required.
                                    }
                                }
                            },
                            "profanityFilterMode": "str",  # Optional. Specifies how to
                              handle profanity in recognition results. Accepted values are None to
                              disable profanity filtering, Masked to replace profanity with asterisks,
                              Removed to remove all profanity from the result, or Tags to add profanity
                              tags. The default value is Masked.
                            "punctuationMode": "str",  # Optional. Specifies how to
                              handle punctuation in recognition results. Accepted values are None to
                              disable punctuation, Dictated to imply explicit (spoken) punctuation,
                              Automatic to let the decoder deal with punctuation, or
                              DictatedAndAutomatic to use dictated and automatic punctuation. The
                              default value is DictatedAndAutomatic.
                            "wordLevelTimestampsEnabled": bool  # Optional. Specifies if
                              word level timestamps should be included in the output. The default value
                              is false.
                        }
                    },
                    "name": "str",  # The unique name of the registration resource. Required.
                    "processingModel": {
                        "deliveryMode": "str",  # deliveryMode. Required.
                        "subjectEndsWith": "str",  # Optional. Extensions of input files.
                        "systemTopicResourceId": "str"  # Optional. ResourceId for System
                          Topic.
                    },
                    "statusModel": {
                        "code": "str",  # Status code. Required.
                        "message": "str"  # Optional. Message.
                    },
                    "storageModel": {
                        "blobStorageEndpoint": "str",  # The Uri of the blob storage account
                          in the form of 'https://:code:`<accountname>`.blob.core.windows.net'.
                          Required.
                        "errorOutputContainerName": "str",  # The name of the container that
                          saves the error messages during processing. Required.
                        "inputContainerName": "str",  # The name of the container that holds
                          the audio files to be processed. Required.
                        "outputContainerName": "str",  # The name of the container that
                          stores the transcription and analytics results. Required.
                        "processedContainerName": "str"  # The name of the container that
                          archives the audio files that have been transcribed. Required.
                    },
                    "analyticsModel": {
                        "endpoint": "str"  # The analytics endpoint to call. Required.
                    }
                }

                # response body for status code(s): 201, 200
                response == {
                    "batchTranscriptionModel": {
                        "customDomain": "str",  # Custom domain name for batch transcription.
                          Required.
                        "locale": "str",  # Optional. The expected locale of the audio data
                          to transcribe. For example, 'en-US', 'fr-FR', 'zh-CN', etc. For a full list
                          of supported locales, see `Supported languages and regions
                          <https://learn.microsoft.com/azure/ai-services/speech-service/language-support?tabs=stt>`_.
                        "modelX": {
                            "self": "str"  # self. Required.
                        },
                        "transcriptionProperties": {
                            "channels": [
                                0  # Optional. An array of channel numbers to
                                  process. Channels 0 and 1 are transcribed by default.
                            ],
                            "diarization": {
                                "speakers": {
                                    "maxCount": 0,  # Optional. Maximum count of
                                      speakers.
                                    "minCount": 0  # Optional. Minimum count of
                                      speakers.
                                }
                            },
                            "diarizationEnabled": bool,  # Optional. Specifies that the
                              Speech service should attempt diarization analysis on the input, which is
                              expected to be a mono channel that contains two voices. The default value
                              is false.
                            "displayFormWordLevelTimestampsEnabled": bool,  # Optional.
                              Specifies whether to include word-level timestamps on the display form of
                              the transcription results. The results are returned in the displayWords
                              property of the transcription file. The default value is false.
                            "languageIdentification": {
                                "candidateLocales": [
                                    "str"  # candidateLocales. Required.
                                ],
                                "mode": "str",  # mode. Required.
                                "speechModelMapping": {
                                    "str": {
                                        "self": "str"  # self. Required.
                                    }
                                }
                            },
                            "profanityFilterMode": "str",  # Optional. Specifies how to
                              handle profanity in recognition results. Accepted values are None to
                              disable profanity filtering, Masked to replace profanity with asterisks,
                              Removed to remove all profanity from the result, or Tags to add profanity
                              tags. The default value is Masked.
                            "punctuationMode": "str",  # Optional. Specifies how to
                              handle punctuation in recognition results. Accepted values are None to
                              disable punctuation, Dictated to imply explicit (spoken) punctuation,
                              Automatic to let the decoder deal with punctuation, or
                              DictatedAndAutomatic to use dictated and automatic punctuation. The
                              default value is DictatedAndAutomatic.
                            "wordLevelTimestampsEnabled": bool  # Optional. Specifies if
                              word level timestamps should be included in the output. The default value
                              is false.
                        }
                    },
                    "name": "str",  # The unique name of the registration resource. Required.
                    "processingModel": {
                        "deliveryMode": "str",  # deliveryMode. Required.
                        "subjectEndsWith": "str",  # Optional. Extensions of input files.
                        "systemTopicResourceId": "str"  # Optional. ResourceId for System
                          Topic.
                    },
                    "statusModel": {
                        "code": "str",  # Status code. Required.
                        "message": "str"  # Optional. Message.
                    },
                    "storageModel": {
                        "blobStorageEndpoint": "str",  # The Uri of the blob storage account
                          in the form of 'https://:code:`<accountname>`.blob.core.windows.net'.
                          Required.
                        "errorOutputContainerName": "str",  # The name of the container that
                          saves the error messages during processing. Required.
                        "inputContainerName": "str",  # The name of the container that holds
                          the audio files to be processed. Required.
                        "outputContainerName": "str",  # The name of the container that
                          stores the transcription and analytics results. Required.
                        "processedContainerName": "str"  # The name of the container that
                          archives the audio files that have been transcribed. Required.
                    },
                    "analyticsModel": {
                        "endpoint": "str"  # The analytics endpoint to call. Required.
                    }
                }
        """

    @overload
    async def create_or_replace_registration(
        self, name: str, resource: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Registration:
        # pylint: disable=line-too-long
        """Creates or replaces a Registration.

        :param name: The unique name of the registration resource. Required.
        :type name: str
        :param resource: The resource instance. Required.
        :type resource: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Registration. The Registration is compatible with MutableMapping
        :rtype: ~speechingestion.models.Registration
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 201, 200
                response == {
                    "batchTranscriptionModel": {
                        "customDomain": "str",  # Custom domain name for batch transcription.
                          Required.
                        "locale": "str",  # Optional. The expected locale of the audio data
                          to transcribe. For example, 'en-US', 'fr-FR', 'zh-CN', etc. For a full list
                          of supported locales, see `Supported languages and regions
                          <https://learn.microsoft.com/azure/ai-services/speech-service/language-support?tabs=stt>`_.
                        "modelX": {
                            "self": "str"  # self. Required.
                        },
                        "transcriptionProperties": {
                            "channels": [
                                0  # Optional. An array of channel numbers to
                                  process. Channels 0 and 1 are transcribed by default.
                            ],
                            "diarization": {
                                "speakers": {
                                    "maxCount": 0,  # Optional. Maximum count of
                                      speakers.
                                    "minCount": 0  # Optional. Minimum count of
                                      speakers.
                                }
                            },
                            "diarizationEnabled": bool,  # Optional. Specifies that the
                              Speech service should attempt diarization analysis on the input, which is
                              expected to be a mono channel that contains two voices. The default value
                              is false.
                            "displayFormWordLevelTimestampsEnabled": bool,  # Optional.
                              Specifies whether to include word-level timestamps on the display form of
                              the transcription results. The results are returned in the displayWords
                              property of the transcription file. The default value is false.
                            "languageIdentification": {
                                "candidateLocales": [
                                    "str"  # candidateLocales. Required.
                                ],
                                "mode": "str",  # mode. Required.
                                "speechModelMapping": {
                                    "str": {
                                        "self": "str"  # self. Required.
                                    }
                                }
                            },
                            "profanityFilterMode": "str",  # Optional. Specifies how to
                              handle profanity in recognition results. Accepted values are None to
                              disable profanity filtering, Masked to replace profanity with asterisks,
                              Removed to remove all profanity from the result, or Tags to add profanity
                              tags. The default value is Masked.
                            "punctuationMode": "str",  # Optional. Specifies how to
                              handle punctuation in recognition results. Accepted values are None to
                              disable punctuation, Dictated to imply explicit (spoken) punctuation,
                              Automatic to let the decoder deal with punctuation, or
                              DictatedAndAutomatic to use dictated and automatic punctuation. The
                              default value is DictatedAndAutomatic.
                            "wordLevelTimestampsEnabled": bool  # Optional. Specifies if
                              word level timestamps should be included in the output. The default value
                              is false.
                        }
                    },
                    "name": "str",  # The unique name of the registration resource. Required.
                    "processingModel": {
                        "deliveryMode": "str",  # deliveryMode. Required.
                        "subjectEndsWith": "str",  # Optional. Extensions of input files.
                        "systemTopicResourceId": "str"  # Optional. ResourceId for System
                          Topic.
                    },
                    "statusModel": {
                        "code": "str",  # Status code. Required.
                        "message": "str"  # Optional. Message.
                    },
                    "storageModel": {
                        "blobStorageEndpoint": "str",  # The Uri of the blob storage account
                          in the form of 'https://:code:`<accountname>`.blob.core.windows.net'.
                          Required.
                        "errorOutputContainerName": "str",  # The name of the container that
                          saves the error messages during processing. Required.
                        "inputContainerName": "str",  # The name of the container that holds
                          the audio files to be processed. Required.
                        "outputContainerName": "str",  # The name of the container that
                          stores the transcription and analytics results. Required.
                        "processedContainerName": "str"  # The name of the container that
                          archives the audio files that have been transcribed. Required.
                    },
                    "analyticsModel": {
                        "endpoint": "str"  # The analytics endpoint to call. Required.
                    }
                }
        """

    @overload
    async def create_or_replace_registration(
        self, name: str, resource: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Registration:
        # pylint: disable=line-too-long
        """Creates or replaces a Registration.

        :param name: The unique name of the registration resource. Required.
        :type name: str
        :param resource: The resource instance. Required.
        :type resource: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Registration. The Registration is compatible with MutableMapping
        :rtype: ~speechingestion.models.Registration
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 201, 200
                response == {
                    "batchTranscriptionModel": {
                        "customDomain": "str",  # Custom domain name for batch transcription.
                          Required.
                        "locale": "str",  # Optional. The expected locale of the audio data
                          to transcribe. For example, 'en-US', 'fr-FR', 'zh-CN', etc. For a full list
                          of supported locales, see `Supported languages and regions
                          <https://learn.microsoft.com/azure/ai-services/speech-service/language-support?tabs=stt>`_.
                        "modelX": {
                            "self": "str"  # self. Required.
                        },
                        "transcriptionProperties": {
                            "channels": [
                                0  # Optional. An array of channel numbers to
                                  process. Channels 0 and 1 are transcribed by default.
                            ],
                            "diarization": {
                                "speakers": {
                                    "maxCount": 0,  # Optional. Maximum count of
                                      speakers.
                                    "minCount": 0  # Optional. Minimum count of
                                      speakers.
                                }
                            },
                            "diarizationEnabled": bool,  # Optional. Specifies that the
                              Speech service should attempt diarization analysis on the input, which is
                              expected to be a mono channel that contains two voices. The default value
                              is false.
                            "displayFormWordLevelTimestampsEnabled": bool,  # Optional.
                              Specifies whether to include word-level timestamps on the display form of
                              the transcription results. The results are returned in the displayWords
                              property of the transcription file. The default value is false.
                            "languageIdentification": {
                                "candidateLocales": [
                                    "str"  # candidateLocales. Required.
                                ],
                                "mode": "str",  # mode. Required.
                                "speechModelMapping": {
                                    "str": {
                                        "self": "str"  # self. Required.
                                    }
                                }
                            },
                            "profanityFilterMode": "str",  # Optional. Specifies how to
                              handle profanity in recognition results. Accepted values are None to
                              disable profanity filtering, Masked to replace profanity with asterisks,
                              Removed to remove all profanity from the result, or Tags to add profanity
                              tags. The default value is Masked.
                            "punctuationMode": "str",  # Optional. Specifies how to
                              handle punctuation in recognition results. Accepted values are None to
                              disable punctuation, Dictated to imply explicit (spoken) punctuation,
                              Automatic to let the decoder deal with punctuation, or
                              DictatedAndAutomatic to use dictated and automatic punctuation. The
                              default value is DictatedAndAutomatic.
                            "wordLevelTimestampsEnabled": bool  # Optional. Specifies if
                              word level timestamps should be included in the output. The default value
                              is false.
                        }
                    },
                    "name": "str",  # The unique name of the registration resource. Required.
                    "processingModel": {
                        "deliveryMode": "str",  # deliveryMode. Required.
                        "subjectEndsWith": "str",  # Optional. Extensions of input files.
                        "systemTopicResourceId": "str"  # Optional. ResourceId for System
                          Topic.
                    },
                    "statusModel": {
                        "code": "str",  # Status code. Required.
                        "message": "str"  # Optional. Message.
                    },
                    "storageModel": {
                        "blobStorageEndpoint": "str",  # The Uri of the blob storage account
                          in the form of 'https://:code:`<accountname>`.blob.core.windows.net'.
                          Required.
                        "errorOutputContainerName": "str",  # The name of the container that
                          saves the error messages during processing. Required.
                        "inputContainerName": "str",  # The name of the container that holds
                          the audio files to be processed. Required.
                        "outputContainerName": "str",  # The name of the container that
                          stores the transcription and analytics results. Required.
                        "processedContainerName": "str"  # The name of the container that
                          archives the audio files that have been transcribed. Required.
                    },
                    "analyticsModel": {
                        "endpoint": "str"  # The analytics endpoint to call. Required.
                    }
                }
        """

    @distributed_trace_async
    async def create_or_replace_registration(
        self, name: str, resource: Union[_models.Registration, JSON, IO[bytes]], **kwargs: Any
    ) -> _models.Registration:
        # pylint: disable=line-too-long
        """Creates or replaces a Registration.

        :param name: The unique name of the registration resource. Required.
        :type name: str
        :param resource: The resource instance. Is one of the following types: Registration, JSON,
         IO[bytes] Required.
        :type resource: ~speechingestion.models.Registration or JSON or IO[bytes]
        :return: Registration. The Registration is compatible with MutableMapping
        :rtype: ~speechingestion.models.Registration
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # JSON input template you can fill out and use as your body input.
                resource = {
                    "batchTranscriptionModel": {
                        "customDomain": "str",  # Custom domain name for batch transcription.
                          Required.
                        "locale": "str",  # Optional. The expected locale of the audio data
                          to transcribe. For example, 'en-US', 'fr-FR', 'zh-CN', etc. For a full list
                          of supported locales, see `Supported languages and regions
                          <https://learn.microsoft.com/azure/ai-services/speech-service/language-support?tabs=stt>`_.
                        "modelX": {
                            "self": "str"  # self. Required.
                        },
                        "transcriptionProperties": {
                            "channels": [
                                0  # Optional. An array of channel numbers to
                                  process. Channels 0 and 1 are transcribed by default.
                            ],
                            "diarization": {
                                "speakers": {
                                    "maxCount": 0,  # Optional. Maximum count of
                                      speakers.
                                    "minCount": 0  # Optional. Minimum count of
                                      speakers.
                                }
                            },
                            "diarizationEnabled": bool,  # Optional. Specifies that the
                              Speech service should attempt diarization analysis on the input, which is
                              expected to be a mono channel that contains two voices. The default value
                              is false.
                            "displayFormWordLevelTimestampsEnabled": bool,  # Optional.
                              Specifies whether to include word-level timestamps on the display form of
                              the transcription results. The results are returned in the displayWords
                              property of the transcription file. The default value is false.
                            "languageIdentification": {
                                "candidateLocales": [
                                    "str"  # candidateLocales. Required.
                                ],
                                "mode": "str",  # mode. Required.
                                "speechModelMapping": {
                                    "str": {
                                        "self": "str"  # self. Required.
                                    }
                                }
                            },
                            "profanityFilterMode": "str",  # Optional. Specifies how to
                              handle profanity in recognition results. Accepted values are None to
                              disable profanity filtering, Masked to replace profanity with asterisks,
                              Removed to remove all profanity from the result, or Tags to add profanity
                              tags. The default value is Masked.
                            "punctuationMode": "str",  # Optional. Specifies how to
                              handle punctuation in recognition results. Accepted values are None to
                              disable punctuation, Dictated to imply explicit (spoken) punctuation,
                              Automatic to let the decoder deal with punctuation, or
                              DictatedAndAutomatic to use dictated and automatic punctuation. The
                              default value is DictatedAndAutomatic.
                            "wordLevelTimestampsEnabled": bool  # Optional. Specifies if
                              word level timestamps should be included in the output. The default value
                              is false.
                        }
                    },
                    "name": "str",  # The unique name of the registration resource. Required.
                    "processingModel": {
                        "deliveryMode": "str",  # deliveryMode. Required.
                        "subjectEndsWith": "str",  # Optional. Extensions of input files.
                        "systemTopicResourceId": "str"  # Optional. ResourceId for System
                          Topic.
                    },
                    "statusModel": {
                        "code": "str",  # Status code. Required.
                        "message": "str"  # Optional. Message.
                    },
                    "storageModel": {
                        "blobStorageEndpoint": "str",  # The Uri of the blob storage account
                          in the form of 'https://:code:`<accountname>`.blob.core.windows.net'.
                          Required.
                        "errorOutputContainerName": "str",  # The name of the container that
                          saves the error messages during processing. Required.
                        "inputContainerName": "str",  # The name of the container that holds
                          the audio files to be processed. Required.
                        "outputContainerName": "str",  # The name of the container that
                          stores the transcription and analytics results. Required.
                        "processedContainerName": "str"  # The name of the container that
                          archives the audio files that have been transcribed. Required.
                    },
                    "analyticsModel": {
                        "endpoint": "str"  # The analytics endpoint to call. Required.
                    }
                }

                # response body for status code(s): 201, 200
                response == {
                    "batchTranscriptionModel": {
                        "customDomain": "str",  # Custom domain name for batch transcription.
                          Required.
                        "locale": "str",  # Optional. The expected locale of the audio data
                          to transcribe. For example, 'en-US', 'fr-FR', 'zh-CN', etc. For a full list
                          of supported locales, see `Supported languages and regions
                          <https://learn.microsoft.com/azure/ai-services/speech-service/language-support?tabs=stt>`_.
                        "modelX": {
                            "self": "str"  # self. Required.
                        },
                        "transcriptionProperties": {
                            "channels": [
                                0  # Optional. An array of channel numbers to
                                  process. Channels 0 and 1 are transcribed by default.
                            ],
                            "diarization": {
                                "speakers": {
                                    "maxCount": 0,  # Optional. Maximum count of
                                      speakers.
                                    "minCount": 0  # Optional. Minimum count of
                                      speakers.
                                }
                            },
                            "diarizationEnabled": bool,  # Optional. Specifies that the
                              Speech service should attempt diarization analysis on the input, which is
                              expected to be a mono channel that contains two voices. The default value
                              is false.
                            "displayFormWordLevelTimestampsEnabled": bool,  # Optional.
                              Specifies whether to include word-level timestamps on the display form of
                              the transcription results. The results are returned in the displayWords
                              property of the transcription file. The default value is false.
                            "languageIdentification": {
                                "candidateLocales": [
                                    "str"  # candidateLocales. Required.
                                ],
                                "mode": "str",  # mode. Required.
                                "speechModelMapping": {
                                    "str": {
                                        "self": "str"  # self. Required.
                                    }
                                }
                            },
                            "profanityFilterMode": "str",  # Optional. Specifies how to
                              handle profanity in recognition results. Accepted values are None to
                              disable profanity filtering, Masked to replace profanity with asterisks,
                              Removed to remove all profanity from the result, or Tags to add profanity
                              tags. The default value is Masked.
                            "punctuationMode": "str",  # Optional. Specifies how to
                              handle punctuation in recognition results. Accepted values are None to
                              disable punctuation, Dictated to imply explicit (spoken) punctuation,
                              Automatic to let the decoder deal with punctuation, or
                              DictatedAndAutomatic to use dictated and automatic punctuation. The
                              default value is DictatedAndAutomatic.
                            "wordLevelTimestampsEnabled": bool  # Optional. Specifies if
                              word level timestamps should be included in the output. The default value
                              is false.
                        }
                    },
                    "name": "str",  # The unique name of the registration resource. Required.
                    "processingModel": {
                        "deliveryMode": "str",  # deliveryMode. Required.
                        "subjectEndsWith": "str",  # Optional. Extensions of input files.
                        "systemTopicResourceId": "str"  # Optional. ResourceId for System
                          Topic.
                    },
                    "statusModel": {
                        "code": "str",  # Status code. Required.
                        "message": "str"  # Optional. Message.
                    },
                    "storageModel": {
                        "blobStorageEndpoint": "str",  # The Uri of the blob storage account
                          in the form of 'https://:code:`<accountname>`.blob.core.windows.net'.
                          Required.
                        "errorOutputContainerName": "str",  # The name of the container that
                          saves the error messages during processing. Required.
                        "inputContainerName": "str",  # The name of the container that holds
                          the audio files to be processed. Required.
                        "outputContainerName": "str",  # The name of the container that
                          stores the transcription and analytics results. Required.
                        "processedContainerName": "str"  # The name of the container that
                          archives the audio files that have been transcribed. Required.
                    },
                    "analyticsModel": {
                        "endpoint": "str"  # The analytics endpoint to call. Required.
                    }
                }
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Registration] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(resource, (IOBase, bytes)):
            _content = resource
        else:
            _content = json.dumps(resource, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_registrations_create_or_replace_registration_request(
            name=name,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 201]:
            if _stream:
                await response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if response.status_code == 200:
            if _stream:
                deserialized = response.iter_bytes()
            else:
                deserialized = _deserialize(_models.Registration, response.json())

        if response.status_code == 201:
            if _stream:
                deserialized = response.iter_bytes()
            else:
                deserialized = _deserialize(_models.Registration, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_registration(  # pylint: disable=inconsistent-return-statements
        self, name: str, **kwargs: Any
    ) -> None:
        """Delete a Registration.

        :param name: The unique name of the registration resource. Required.
        :type name: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        _request = build_registrations_delete_registration_request(
            name=name,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = False
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [204]:
            if _stream:
                await response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if cls:
            return cls(pipeline_response, None, {})  # type: ignore

    @distributed_trace
    def list_registrations(self, **kwargs: Any) -> AsyncIterable["_models.Registration"]:
        # pylint: disable=line-too-long
        """List Registration resources.

        :return: An iterator like instance of Registration
        :rtype: ~azure.core.async_paging.AsyncItemPaged[~speechingestion.models.Registration]
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "batchTranscriptionModel": {
                        "customDomain": "str",  # Custom domain name for batch transcription.
                          Required.
                        "locale": "str",  # Optional. The expected locale of the audio data
                          to transcribe. For example, 'en-US', 'fr-FR', 'zh-CN', etc. For a full list
                          of supported locales, see `Supported languages and regions
                          <https://learn.microsoft.com/azure/ai-services/speech-service/language-support?tabs=stt>`_.
                        "modelX": {
                            "self": "str"  # self. Required.
                        },
                        "transcriptionProperties": {
                            "channels": [
                                0  # Optional. An array of channel numbers to
                                  process. Channels 0 and 1 are transcribed by default.
                            ],
                            "diarization": {
                                "speakers": {
                                    "maxCount": 0,  # Optional. Maximum count of
                                      speakers.
                                    "minCount": 0  # Optional. Minimum count of
                                      speakers.
                                }
                            },
                            "diarizationEnabled": bool,  # Optional. Specifies that the
                              Speech service should attempt diarization analysis on the input, which is
                              expected to be a mono channel that contains two voices. The default value
                              is false.
                            "displayFormWordLevelTimestampsEnabled": bool,  # Optional.
                              Specifies whether to include word-level timestamps on the display form of
                              the transcription results. The results are returned in the displayWords
                              property of the transcription file. The default value is false.
                            "languageIdentification": {
                                "candidateLocales": [
                                    "str"  # candidateLocales. Required.
                                ],
                                "mode": "str",  # mode. Required.
                                "speechModelMapping": {
                                    "str": {
                                        "self": "str"  # self. Required.
                                    }
                                }
                            },
                            "profanityFilterMode": "str",  # Optional. Specifies how to
                              handle profanity in recognition results. Accepted values are None to
                              disable profanity filtering, Masked to replace profanity with asterisks,
                              Removed to remove all profanity from the result, or Tags to add profanity
                              tags. The default value is Masked.
                            "punctuationMode": "str",  # Optional. Specifies how to
                              handle punctuation in recognition results. Accepted values are None to
                              disable punctuation, Dictated to imply explicit (spoken) punctuation,
                              Automatic to let the decoder deal with punctuation, or
                              DictatedAndAutomatic to use dictated and automatic punctuation. The
                              default value is DictatedAndAutomatic.
                            "wordLevelTimestampsEnabled": bool  # Optional. Specifies if
                              word level timestamps should be included in the output. The default value
                              is false.
                        }
                    },
                    "name": "str",  # The unique name of the registration resource. Required.
                    "processingModel": {
                        "deliveryMode": "str",  # deliveryMode. Required.
                        "subjectEndsWith": "str",  # Optional. Extensions of input files.
                        "systemTopicResourceId": "str"  # Optional. ResourceId for System
                          Topic.
                    },
                    "statusModel": {
                        "code": "str",  # Status code. Required.
                        "message": "str"  # Optional. Message.
                    },
                    "storageModel": {
                        "blobStorageEndpoint": "str",  # The Uri of the blob storage account
                          in the form of 'https://:code:`<accountname>`.blob.core.windows.net'.
                          Required.
                        "errorOutputContainerName": "str",  # The name of the container that
                          saves the error messages during processing. Required.
                        "inputContainerName": "str",  # The name of the container that holds
                          the audio files to be processed. Required.
                        "outputContainerName": "str",  # The name of the container that
                          stores the transcription and analytics results. Required.
                        "processedContainerName": "str"  # The name of the container that
                          archives the audio files that have been transcribed. Required.
                    },
                    "analyticsModel": {
                        "endpoint": "str"  # The analytics endpoint to call. Required.
                    }
                }
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[List[_models.Registration]] = kwargs.pop("cls", None)

        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_registrations_list_registrations_request(
                    api_version=self._config.api_version,
                    headers=_headers,
                    params=_params,
                )
                _request.url = self._client.format_url(_request.url)

            else:
                # make call to next link with the client's api-version
                _parsed_next_link = urllib.parse.urlparse(next_link)
                _next_request_params = case_insensitive_dict(
                    {
                        key: [urllib.parse.quote(v) for v in value]
                        for key, value in urllib.parse.parse_qs(_parsed_next_link.query).items()
                    }
                )
                _next_request_params["api-version"] = self._config.api_version
                _request = HttpRequest(
                    "GET", urllib.parse.urljoin(next_link, _parsed_next_link.path), params=_next_request_params
                )
                _request.url = self._client.format_url(_request.url)

            return _request

        async def extract_data(pipeline_response):
            deserialized = pipeline_response.http_response.json()
            list_of_elem = _deserialize(List[_models.Registration], deserialized["value"])
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return deserialized.get("nextLink") or None, AsyncList(list_of_elem)

        async def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                if _stream:
                    await response.read()  # Load the body in memory and close the socket
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                raise HttpResponseError(response=response)

            return pipeline_response

        return AsyncItemPaged(get_next, extract_data)
