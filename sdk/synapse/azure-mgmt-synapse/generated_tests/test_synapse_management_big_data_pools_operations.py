# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
import pytest
from azure.mgmt.synapse import SynapseManagementClient

from devtools_testutils import AzureMgmtRecordedTestCase, RandomNameResourceGroupPreparer, recorded_by_proxy

AZURE_LOCATION = "eastus"


@pytest.mark.skip("you may need to update the auto-generated test case before run it")
class TestSynapseManagementBigDataPoolsOperations(AzureMgmtRecordedTestCase):
    def setup_method(self, method):
        self.client = self.create_mgmt_client(SynapseManagementClient)

    @RandomNameResourceGroupPreparer(location=AZURE_LOCATION)
    @recorded_by_proxy
    def test_big_data_pools_get(self, resource_group):
        response = self.client.big_data_pools.get(
            resource_group_name=resource_group.name,
            workspace_name="str",
            big_data_pool_name="str",
            api_version="2021-06-01-preview",
        )

        # please add some check logic here by yourself
        # ...

    @RandomNameResourceGroupPreparer(location=AZURE_LOCATION)
    @recorded_by_proxy
    def test_big_data_pools_update(self, resource_group):
        response = self.client.big_data_pools.update(
            resource_group_name=resource_group.name,
            workspace_name="str",
            big_data_pool_name="str",
            big_data_pool_patch_info={"tags": {"str": "str"}},
            api_version="2021-06-01-preview",
        )

        # please add some check logic here by yourself
        # ...

    @RandomNameResourceGroupPreparer(location=AZURE_LOCATION)
    @recorded_by_proxy
    def test_big_data_pools_begin_create_or_update(self, resource_group):
        response = self.client.big_data_pools.begin_create_or_update(
            resource_group_name=resource_group.name,
            workspace_name="str",
            big_data_pool_name="str",
            big_data_pool_info={
                "location": "str",
                "autoPause": {"delayInMinutes": 0, "enabled": bool},
                "autoScale": {"enabled": bool, "maxNodeCount": 0, "minNodeCount": 0},
                "cacheSize": 0,
                "creationDate": "2020-02-20 00:00:00",
                "customLibraries": [
                    {
                        "containerName": "str",
                        "creatorId": "str",
                        "name": "str",
                        "path": "str",
                        "provisioningStatus": "str",
                        "type": "str",
                        "uploadedTimestamp": "2020-02-20 00:00:00",
                    }
                ],
                "defaultSparkLogFolder": "str",
                "dynamicExecutorAllocation": {"enabled": bool, "maxExecutors": 0, "minExecutors": 0},
                "id": "str",
                "isAutotuneEnabled": bool,
                "isComputeIsolationEnabled": bool,
                "lastSucceededTimestamp": "2020-02-20 00:00:00",
                "libraryRequirements": {"content": "str", "filename": "str", "time": "2020-02-20 00:00:00"},
                "name": "str",
                "nodeCount": 0,
                "nodeSize": "str",
                "nodeSizeFamily": "str",
                "provisioningState": "str",
                "sessionLevelPackagesEnabled": bool,
                "sparkConfigProperties": {
                    "configurationType": "str",
                    "content": "str",
                    "filename": "str",
                    "time": "2020-02-20 00:00:00",
                },
                "sparkEventsFolder": "str",
                "sparkVersion": "str",
                "tags": {"str": "str"},
                "type": "str",
            },
            api_version="2021-06-01-preview",
        ).result()  # call '.result()' to poll until service return final result

        # please add some check logic here by yourself
        # ...

    @RandomNameResourceGroupPreparer(location=AZURE_LOCATION)
    @recorded_by_proxy
    def test_big_data_pools_begin_delete(self, resource_group):
        response = self.client.big_data_pools.begin_delete(
            resource_group_name=resource_group.name,
            workspace_name="str",
            big_data_pool_name="str",
            api_version="2021-06-01-preview",
        ).result()  # call '.result()' to poll until service return final result

        # please add some check logic here by yourself
        # ...

    @RandomNameResourceGroupPreparer(location=AZURE_LOCATION)
    @recorded_by_proxy
    def test_big_data_pools_list_by_workspace(self, resource_group):
        response = self.client.big_data_pools.list_by_workspace(
            resource_group_name=resource_group.name,
            workspace_name="str",
            api_version="2021-06-01-preview",
        )
        result = [r for r in response]
        # please add some check logic here by yourself
        # ...
