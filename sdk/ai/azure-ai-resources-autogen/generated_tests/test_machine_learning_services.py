# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
import pytest
from devtools_testutils import recorded_by_proxy
from testpreparer import MachineLearningServicesClientTestBase, MachineLearningServicesPreparer


@pytest.mark.skip("you may need to update the auto-generated test case before run it")
class TestMachineLearningServices(MachineLearningServicesClientTestBase):
    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_assistant(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_assistant(
            body={
                "model": "str",
                "description": "str",
                "instructions": "str",
                "metadata": {"str": "str"},
                "name": "str",
                "response_format": "str",
                "temperature": 0.0,
                "tool_resources": {"code_interpreter": {"file_ids": ["str"]}, "file_search": ["str"]},
                "tools": ["tool_definition"],
                "top_p": 0.0,
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_assistants(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_assistants()

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_assistant(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_assistant(
            assistant_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_update_assistant(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.update_assistant(
            assistant_id="str",
            body={
                "description": "str",
                "instructions": "str",
                "metadata": {"str": "str"},
                "model": "str",
                "name": "str",
                "response_format": "str",
                "temperature": 0.0,
                "tool_resources": {
                    "code_interpreter": {"file_ids": ["str"]},
                    "file_search": {"vector_store_ids": ["str"]},
                },
                "tools": ["tool_definition"],
                "top_p": 0.0,
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_delete_assistant(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.delete_assistant(
            assistant_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_files(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_files()

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_upload_file(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.upload_file(
            body={"file": "filetype", "purpose": "str", "filename": "str"},
            file="filetype",
            purpose="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_delete_file(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.delete_file(
            file_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_file(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_file(
            file_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_file_content(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_file_content(
            file_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_online_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_online_endpoint(
            name="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_delete_online_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.delete_online_endpoint(
            name="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_update_online_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.update_online_endpoint(
            name="str",
            body={"identity": {"type": "str", "userAssignedIdentities": {"str": {"str": {}}}}, "tags": {"str": "str"}},
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_or_update_online_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_or_update_online_endpoint(
            name="str",
            body={
                "authMode": "str",
                "compute": "str",
                "description": "str",
                "keys": {"primaryKey": "str", "secondaryKey": "str"},
                "mirrorTraffic": {"str": 0},
                "properties": {"str": "str"},
                "provisioningState": "str",
                "publicNetworkAccess": "str",
                "scoringUri": "str",
                "swaggerUri": "str",
                "traffic": {"str": 0},
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_online_endpoints(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_online_endpoints()
        result = [r for r in response]
        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_keys_online_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_keys_online_endpoint(
            name="str",
            body={"keyType": "str", "keyValue": "str"},
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_regenerate_keys_online_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.regenerate_keys_online_endpoint(
            name="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_token_online_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_token_online_endpoint(
            name="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_online_deployments(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_online_deployments(
            endpoint_name="str",
        )
        result = [r for r in response]
        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_delete_online_deployment(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.delete_online_deployment(
            endpoint_name="str",
            deployment_name="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_online_deployment(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_online_deployment(
            endpoint_name="str",
            deployment_name="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_update_online_deployment(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.update_online_deployment(
            endpoint_name="str",
            deployment_name="str",
            body={
                "sku": {"capacity": 0, "family": "str", "name": "str", "size": "str", "tier": "str"},
                "tags": {"str": "str"},
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_or_update_online_deployment(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_or_update_online_deployment(
            endpoint_name="str",
            deployment_name="str",
            body={
                "appInsightsEnabled": bool,
                "codeConfiguration": {"scoringScript": "str", "codeId": "str"},
                "dataCollector": {
                    "collections": {
                        "str": {"clientId": "str", "dataCollectionMode": "str", "dataId": "str", "samplingRate": 0.0}
                    },
                    "requestLogging": {"captureHeaders": ["str"]},
                    "rollingRate": "str",
                },
                "description": "str",
                "egressPublicNetworkAccess": "str",
                "environmentId": "str",
                "environmentVariables": {"str": "str"},
                "instanceType": "str",
                "livenessProbe": {
                    "failureThreshold": 0,
                    "initialDelay": "1 day, 0:00:00",
                    "period": "1 day, 0:00:00",
                    "successThreshold": 0,
                    "timeout": "1 day, 0:00:00",
                },
                "model": "str",
                "modelMountPath": "str",
                "properties": {"str": "str"},
                "provisioningState": "str",
                "readinessProbe": {
                    "failureThreshold": 0,
                    "initialDelay": "1 day, 0:00:00",
                    "period": "1 day, 0:00:00",
                    "successThreshold": 0,
                    "timeout": "1 day, 0:00:00",
                },
                "requestSettings": {
                    "maxConcurrentRequestsPerInstance": 0,
                    "maxQueueWait": "1 day, 0:00:00",
                    "requestTimeout": "1 day, 0:00:00",
                },
                "scaleSettings": {},
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_poll_logs_online_deployment(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.poll_logs_online_deployment(
            endpoint_name="str",
            deployment_name="str",
            body={"containerType": "str", "tail": 0},
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_skus_online_deployment(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_skus_online_deployment(
            endpoint_name="str",
            deployment_name="str",
            count=0,
        )
        result = [r for r in response]
        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_batch_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_batch_endpoint(
            endpoint_name="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_delete_batch_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.delete_batch_endpoint(
            endpoint_name="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_update_batch_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.update_batch_endpoint(
            endpoint_name="str",
            body={"identity": {"type": "str", "userAssignedIdentities": {"str": {"str": {}}}}, "tags": {"str": "str"}},
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_or_update_batch_endpoint(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_or_update_batch_endpoint(
            endpoint_name="str",
            body={
                "authMode": "str",
                "defaults": {"deploymentName": "str"},
                "description": "str",
                "keys": {"primaryKey": "str", "secondaryKey": "str"},
                "properties": {"str": "str"},
                "provisioningState": "str",
                "scoringUri": "str",
                "swaggerUri": "str",
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_batch_endpoints(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_batch_endpoints()
        result = [r for r in response]
        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_batch_deployments(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_batch_deployments(
            endpoint_name="str",
        )
        result = [r for r in response]
        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_delete_batch_deployment(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.delete_batch_deployment(
            endpoint_name="str",
            deployment_name="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_batch_deployment(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_batch_deployment(
            endpoint_name="str",
            deployment_name="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_update_batch_deployment(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.update_batch_deployment(
            endpoint_name="str",
            deployment_name="str",
            body={"properties": {"description": "str"}, "tags": {"str": "str"}},
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_or_update_batch_deployment(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_or_update_batch_deployment(
            endpoint_name="str",
            deployment_name="str",
            body={
                "codeConfiguration": {"scoringScript": "str", "codeId": "str"},
                "compute": "str",
                "deploymentConfiguration": {},
                "description": "str",
                "environmentId": "str",
                "environmentVariables": {"str": "str"},
                "errorThreshold": 0,
                "loggingLevel": "str",
                "maxConcurrencyPerInstance": 0,
                "miniBatchSize": 0,
                "model": "asset_reference_base",
                "outputAction": "str",
                "outputFileName": "str",
                "properties": {"str": "str"},
                "provisioningState": "str",
                "resources": {"instanceCount": 0, "instanceType": "str", "properties": {"str": {"str": {}}}},
                "retrySettings": {"maxRetries": 0, "timeout": "1 day, 0:00:00"},
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_message(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_message(
            thread_id="str",
            body={
                "content": "str",
                "role": "str",
                "attachments": [{"file_id": "str", "tools": [{"type": "code_interpreter"}]}],
                "metadata": {"str": "str"},
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_messages(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_messages(
            thread_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_message(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_message(
            thread_id="str",
            message_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_update_message(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.update_message(
            thread_id="str",
            message_id="str",
            body={"metadata": {"str": "str"}},
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_run_step(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_run_step(
            thread_id="str",
            run_id="str",
            step_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_run_steps(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_run_steps(
            thread_id="str",
            run_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_run(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_run(
            thread_id="str",
            body={
                "assistant_id": "str",
                "additional_instructions": "str",
                "additional_messages": [
                    {
                        "assistant_id": "str",
                        "attachments": [{"file_id": "str", "tools": [{"type": "code_interpreter"}]}],
                        "completed_at": "2020-02-20 00:00:00",
                        "content": ["message_content"],
                        "created_at": "2020-02-20 00:00:00",
                        "id": "str",
                        "incomplete_at": "2020-02-20 00:00:00",
                        "incomplete_details": {"reason": "str"},
                        "metadata": {"str": "str"},
                        "object": "thread.message",
                        "role": "str",
                        "run_id": "str",
                        "status": "str",
                        "thread_id": "str",
                    }
                ],
                "instructions": "str",
                "max_completion_tokens": 0,
                "max_prompt_tokens": 0,
                "metadata": {"str": "str"},
                "model": "str",
                "response_format": "str",
                "stream": bool,
                "temperature": 0.0,
                "tool_choice": "str",
                "tools": ["tool_definition"],
                "top_p": 0.0,
                "truncation_strategy": {"type": "str", "last_messages": 0},
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_runs(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_runs(
            thread_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_run(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_run(
            thread_id="str",
            run_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_update_run(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.update_run(
            thread_id="str",
            run_id="str",
            body={"metadata": {"str": "str"}},
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_submit_tool_outputs_to_run(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.submit_tool_outputs_to_run(
            thread_id="str",
            run_id="str",
            body={"tool_outputs": [{"output": "str", "tool_call_id": "str"}], "stream": bool},
            tool_outputs=[{"output": "str", "tool_call_id": "str"}],
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_cancel_run(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.cancel_run(
            thread_id="str",
            run_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_thread_and_run(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_thread_and_run(
            body={
                "assistant_id": "str",
                "instructions": "str",
                "max_completion_tokens": 0,
                "max_prompt_tokens": 0,
                "metadata": {"str": "str"},
                "model": "str",
                "response_format": "str",
                "stream": bool,
                "temperature": 0.0,
                "thread": {
                    "messages": [
                        {
                            "content": "str",
                            "role": "str",
                            "attachments": [{"file_id": "str", "tools": [{"type": "code_interpreter"}]}],
                            "metadata": {"str": "str"},
                        }
                    ],
                    "metadata": {"str": "str"},
                    "tool_resources": {"code_interpreter": {"file_ids": ["str"]}, "file_search": ["str"]},
                },
                "tool_choice": "str",
                "tool_resources": {
                    "code_interpreter": {"file_ids": ["str"]},
                    "file_search": {"vector_store_ids": ["str"]},
                },
                "tools": ["tool_definition"],
                "top_p": 0.0,
                "truncation_strategy": {"type": "str", "last_messages": 0},
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_thread(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_thread(
            body={
                "messages": [
                    {
                        "content": "str",
                        "role": "str",
                        "attachments": [{"file_id": "str", "tools": [{"type": "code_interpreter"}]}],
                        "metadata": {"str": "str"},
                    }
                ],
                "metadata": {"str": "str"},
                "tool_resources": {"code_interpreter": {"file_ids": ["str"]}, "file_search": ["str"]},
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_thread(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_thread(
            thread_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_update_thread(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.update_thread(
            thread_id="str",
            body={
                "metadata": {"str": "str"},
                "tool_resources": {
                    "code_interpreter": {"file_ids": ["str"]},
                    "file_search": {"vector_store_ids": ["str"]},
                },
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_delete_thread(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.delete_thread(
            thread_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_vector_stores(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_vector_stores()

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_vector_store(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_vector_store(
            body={
                "chunking_strategy": "vector_store_chunking_strategy_request",
                "expires_after": {"anchor": "str", "days": 0},
                "file_ids": ["str"],
                "metadata": {"str": "str"},
                "name": "str",
            },
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_vector_store(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_vector_store(
            vector_store_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_modify_vector_store(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.modify_vector_store(
            vector_store_id="str",
            body={"expires_after": {"anchor": "str", "days": 0}, "metadata": {"str": "str"}, "name": "str"},
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_delete_vector_store(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.delete_vector_store(
            vector_store_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_vector_store_files(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_vector_store_files(
            vector_store_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_vector_store_file(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_vector_store_file(
            vector_store_id="str",
            body={"file_id": "str", "chunking_strategy": "vector_store_chunking_strategy_request"},
            file_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_vector_store_file(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_vector_store_file(
            vector_store_id="str",
            file_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_delete_vector_store_file(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.delete_vector_store_file(
            vector_store_id="str",
            file_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_create_vector_store_file_batch(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.create_vector_store_file_batch(
            vector_store_id="str",
            body={"file_ids": ["str"], "chunking_strategy": "vector_store_chunking_strategy_request"},
            file_ids=["str"],
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_get_vector_store_file_batch(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.get_vector_store_file_batch(
            vector_store_id="str",
            batch_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_cancel_vector_store_file_batch(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.cancel_vector_store_file_batch(
            vector_store_id="str",
            batch_id="str",
        )

        # please add some check logic here by yourself
        # ...

    @MachineLearningServicesPreparer()
    @recorded_by_proxy
    def test_list_vector_store_file_batch_files(self, machinelearningservices_endpoint):
        client = self.create_client(endpoint=machinelearningservices_endpoint)
        response = client.list_vector_store_file_batch_files(
            vector_store_id="str",
            batch_id="str",
        )

        # please add some check logic here by yourself
        # ...
