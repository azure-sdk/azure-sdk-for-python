# pylint: disable=too-many-lines,too-many-statements
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from io import IOBase
import json
import sys
from typing import Any, Callable, Dict, IO, Optional, TypeVar, Union, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    map_error,
)
from azure.core.pipeline import PipelineResponse
from azure.core.rest import AsyncHttpResponse, HttpRequest
from azure.core.tracing.decorator_async import distributed_trace_async
from azure.core.utils import case_insensitive_dict

from ... import models as _models
from ..._model_base import SdkJSONEncoder, _deserialize
from ..._operations._operations import build_model_get_chat_completions_request
from .._vendor import ModelClientMixinABC

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
JSON = MutableMapping[str, Any]  # pylint: disable=unsubscriptable-object
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]


class ModelClientOperationsMixin(ModelClientMixinABC):
    @overload
    async def get_chat_completions(
        self,
        chat_completion_options: _models.ChatCompletionsOptions,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param chat_completion_options: The JSON payload containing chat completion options. Required.
        :type chat_completion_options: ~azure.ai.inference.models.ChatCompletionsOptions
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # The input is polymorphic. The following are possible polymorphic inputs based off
                  discriminator "type":

                # JSON input template for discriminator value "json_object":
                chat_completions_response_format = {
                    "type": "json_object"
                }

                # JSON input template for discriminator value "text":
                chat_completions_response_format = {
                    "type": "text"
                }

                # JSON input template you can fill out and use as your body input.
                chat_completion_options = {
                    "messages": [
                        chat_request_message
                    ],
                    "frequency_penalty": 0.0,  # Optional. A value that influences the
                      probability of generated tokens appearing based on their cumulative frequency in
                      generated text. Positive values will make tokens less likely to appear as their
                      frequency increases and decrease the likelihood of the model repeating the same
                      statements verbatim.
                    "max_tokens": 0,  # Optional. The maximum number of tokens to generate.
                    "presence_penalty": 0.0,  # Optional. A value that influences the probability
                      of generated tokens appearing based on their existing presence in generated text.
                      Positive values will make tokens less likely to appear when they already exist
                      and increase the model's likelihood to output new topics.
                    "response_format": chat_completions_response_format,
                    "seed": 0,  # Optional. If specified, the system will make a best effort to
                      sample deterministically such that repeated requests with the same seed and
                      parameters should return the same result. Determinism is not guaranteed, and you
                      should refer to the system_fingerprint response parameter to monitor changes in
                      the backend.".
                    "stop": [
                        "str"  # Optional. A collection of textual sequences that will end
                          completions generation.
                    ],
                    "stream": bool,  # Optional. A value indicating whether chat completions
                      should be streamed for this request.
                    "temperature": 0.0,  # Optional. The sampling temperature to use that
                      controls the apparent creativity of generated completions. Higher values will
                      make output more random while lower values will make results more focused and
                      deterministic. It is not recommended to modify temperature and top_p for the same
                      completions request as the interaction of these two settings is difficult to
                      predict.
                    "top_p": 0.0  # Optional. An alternative to sampling with temperature called
                      nucleus sampling. This value causes the model to consider the results of tokens
                      with the provided probability mass. As an example, a value of 0.15 will cause
                      only the tokens comprising the top 15% of probability mass to be considered. It
                      is not recommended to modify temperature and top_p for the same completions
                      request as the interaction of these two settings is difficult to predict.
                }

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "delta": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str"  # The chat role associated with the
                                  message. Required. Known values are: "system", "assistant", and
                                  "user".
                            },
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str"  # The chat role associated with the
                                  message. Required. Known values are: "system", "assistant", and
                                  "user".
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """

    @overload
    async def get_chat_completions(
        self, chat_completion_options: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param chat_completion_options: The JSON payload containing chat completion options. Required.
        :type chat_completion_options: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "delta": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str"  # The chat role associated with the
                                  message. Required. Known values are: "system", "assistant", and
                                  "user".
                            },
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str"  # The chat role associated with the
                                  message. Required. Known values are: "system", "assistant", and
                                  "user".
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """

    @overload
    async def get_chat_completions(
        self, chat_completion_options: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param chat_completion_options: The JSON payload containing chat completion options. Required.
        :type chat_completion_options: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "delta": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str"  # The chat role associated with the
                                  message. Required. Known values are: "system", "assistant", and
                                  "user".
                            },
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str"  # The chat role associated with the
                                  message. Required. Known values are: "system", "assistant", and
                                  "user".
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """

    @distributed_trace_async
    async def get_chat_completions(
        self, chat_completion_options: Union[_models.ChatCompletionsOptions, JSON, IO[bytes]], **kwargs: Any
    ) -> _models.ChatCompletions:
        # pylint: disable=line-too-long
        """Gets chat completions for the provided chat messages.
        Completions support a wide variety of tasks and generate text that continues from or
        "completes"
        provided prompt data.

        :param chat_completion_options: The JSON payload containing chat completion options. Is one of
         the following types: ChatCompletionsOptions, JSON, IO[bytes] Required.
        :type chat_completion_options: ~azure.ai.inference.models.ChatCompletionsOptions or JSON or
         IO[bytes]
        :return: ChatCompletions. The ChatCompletions is compatible with MutableMapping
        :rtype: ~azure.ai.inference.models.ChatCompletions
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # The input is polymorphic. The following are possible polymorphic inputs based off
                  discriminator "type":

                # JSON input template for discriminator value "json_object":
                chat_completions_response_format = {
                    "type": "json_object"
                }

                # JSON input template for discriminator value "text":
                chat_completions_response_format = {
                    "type": "text"
                }

                # JSON input template you can fill out and use as your body input.
                chat_completion_options = {
                    "messages": [
                        chat_request_message
                    ],
                    "frequency_penalty": 0.0,  # Optional. A value that influences the
                      probability of generated tokens appearing based on their cumulative frequency in
                      generated text. Positive values will make tokens less likely to appear as their
                      frequency increases and decrease the likelihood of the model repeating the same
                      statements verbatim.
                    "max_tokens": 0,  # Optional. The maximum number of tokens to generate.
                    "presence_penalty": 0.0,  # Optional. A value that influences the probability
                      of generated tokens appearing based on their existing presence in generated text.
                      Positive values will make tokens less likely to appear when they already exist
                      and increase the model's likelihood to output new topics.
                    "response_format": chat_completions_response_format,
                    "seed": 0,  # Optional. If specified, the system will make a best effort to
                      sample deterministically such that repeated requests with the same seed and
                      parameters should return the same result. Determinism is not guaranteed, and you
                      should refer to the system_fingerprint response parameter to monitor changes in
                      the backend.".
                    "stop": [
                        "str"  # Optional. A collection of textual sequences that will end
                          completions generation.
                    ],
                    "stream": bool,  # Optional. A value indicating whether chat completions
                      should be streamed for this request.
                    "temperature": 0.0,  # Optional. The sampling temperature to use that
                      controls the apparent creativity of generated completions. Higher values will
                      make output more random while lower values will make results more focused and
                      deterministic. It is not recommended to modify temperature and top_p for the same
                      completions request as the interaction of these two settings is difficult to
                      predict.
                    "top_p": 0.0  # Optional. An alternative to sampling with temperature called
                      nucleus sampling. This value causes the model to consider the results of tokens
                      with the provided probability mass. As an example, a value of 0.15 will cause
                      only the tokens comprising the top 15% of probability mass to be considered. It
                      is not recommended to modify temperature and top_p for the same completions
                      request as the interaction of these two settings is difficult to predict.
                }

                # response body for status code(s): 200
                response == {
                    "choices": [
                        {
                            "finish_reason": "str",  # The reason that this chat
                              completions choice completed its generated. Required. Known values are:
                              "stop", "length", and "content_filter".
                            "index": 0,  # The ordered index associated with this chat
                              completions choice. Required.
                            "delta": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str"  # The chat role associated with the
                                  message. Required. Known values are: "system", "assistant", and
                                  "user".
                            },
                            "message": {
                                "content": "str",  # The content of the message.
                                  Required.
                                "role": "str"  # The chat role associated with the
                                  message. Required. Known values are: "system", "assistant", and
                                  "user".
                            }
                        }
                    ],
                    "created": "2020-02-20 00:00:00",  # The first timestamp associated with
                      generation activity for this completions response, represented as seconds since
                      the beginning of the Unix epoch of 00:00 on 1 Jan 1970. Required.
                    "id": "str",  # A unique identifier associated with this chat completions
                      response. Required.
                    "model": "str",  # The model used for the chat completion. Required.
                    "object": "str",  # The response object type, which is always
                      ``chat.completion``. Required.
                    "usage": {
                        "completion_tokens": 0,  # The number of tokens generated across all
                          completions emissions. Required.
                        "prompt_tokens": 0,  # The number of tokens in the provided prompts
                          for the completions request. Required.
                        "total_tokens": 0  # The total number of tokens processed for the
                          completions request and response. Required.
                    }
                }
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("content-type", None))
        cls: ClsType[_models.ChatCompletions] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(chat_completion_options, (IOBase, bytes)):
            _content = chat_completion_options
        else:
            _content = json.dumps(chat_completion_options, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_model_get_chat_completions_request(
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        _request.url = self._client.format_url(_request.url)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                await response.read()  # Load the body in memory and close the socket
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models.ChatCompletions, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore
